# flappyAI
A repo for my CS474 final project, a flappy bird deep neural network AI

Demo of the original game: https://flappybird.io/

# Project Specification:
### Project Description:

**Project Proposal: Training a Game-Playing AI for Flappy Bird**

### **Objective**

The objective of this project is to design and train a reinforcement learning (RL) agent to play a clone of the popular game Flappy Bird. The agent will learn through trial and error using a deep Q-network (DQN), with the goal of maximizing its survival time by successfully navigating through pipes. All gameplay data will be generated dynamically during the simulated training process.

### **Tools and Environments**

1. **Game Environment:** PyGame (to create a clone of Flappy Bird).
2. **Programming Language:** Python.
3. **Deep Learning Framework:** PyTorch or TensorFlow for building and training the neural network.
4. **RL Library:** Stable-Baselines3 for implementation of RL algorithms.
5. **Visualization:** Matplotlib and PyGame for tracking progress and generating visual outputs of training performance.

### **Methodology**

### **Game Design and Environment Setup (5 hours)**

- Create or adapt a Flappy Bird clone using PyGame.
- Define the state space: The bird's position, velocity, and the relative positions of the nearest pipe pair.
- Define the action space: Flap (jump) or do nothing.
- Create interfaces to reset the game and step through time, providing rewards and observing new states.

### **Model Architecture (4 hours)**

- Design a deep Q-network with:
    - Input: A fixed-size state vector representing the game environment.
    - Hidden layers: 2-3 fully connected layers with ReLU activation.
    - Output: Q-values for each possible action (flap or no flap).
- Potential hyperparameters:
    - Learning rate: 0.001
    - Discount factor (γ): 0.99
    - Batch size: 32
    - Replay buffer size: 50,000 experiences.

### **Training Strategy (15 hours)**

1. **Exploration vs. Exploitation:**
    - Use ε-greedy exploration with an initial ε of 1.0, decaying over time to 0.1.
    - This ensures that the agent starts by exploring random actions and gradually shifts toward exploiting its learned policy.
2. **Replay Buffer:**
    - Store gameplay experiences (state, action, reward, next state) in a replay buffer.
    - Train the network by sampling mini-batches from the buffer, which helps stabilize learning by breaking correlations between consecutive experiences.
3. **Rewards and Punishments:**
    - Reward: +1 for passing a pipe.
    - Punishment: -1 for hitting a pipe or falling out of bounds.
    - Living reward: 0 (neutral) for staying alive.

### **Progress Tracking (4 hours)**

- Track key metrics during training:
    - Episode length (number of pipes passed).
    - Average reward per episode.
    - Loss values during gradient updates.
- Visualize the agent’s gameplay periodically to observe its progress.
- Generate plots to show the improvement of average scores over time.

### **Testing and Fine-Tuning (4 hours)**

- Evaluate the trained agent on unseen game instances.
- Adjust hyperparameters, reward structure, or model architecture if the performance plateaus or fails to improve.
- Introduce additional game challenges (e.g., varying pipe gaps or speeds) to test robustness.

### **Data Source**

All data for training will be generated by the simulated gameplay environment. The agent interacts with the PyGame Flappy Bird environment to collect states, actions, and rewards dynamically.

### **Time Estimate**

1. **Environment Setup:** 5 hours
2. **Model Architecture Design:** 4 hours
3. **Training and Debugging:** 15 hours
4. **Progress Tracking and Visualization:** 4 hours
5. **Testing and Fine-Tuning:** 4 hours **Total:** ~32 hours (excluding actual training time).

### **Expected Outcome**

The project will produce an RL agent capable of playing Flappy Bird with a strategy learned from scratch. The results will include:

- A trained DQN model.
- Visualizations of the agent’s performance over time.
- Insights into the effectiveness of RL techniques like ε-greedy exploration and replay buffers.

This project offers an engaging application of reinforcement learning and demonstrates the process of training an agent to solve a real-time control problem.