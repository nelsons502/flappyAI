# flappyAI
A repo for my CS474 final project, a flappy bird deep neural network AI

Demo of the original game: https://flappybird.io/

# Project Specification:
### Project Description:

**Project Proposal: Training a Game-Playing AI for Flappy Bird**

### **Objective**

The objective of this project is to design and train a reinforcement learning (RL) agent to play a clone of the popular game Flappy Bird. The agent will learn through trial and error using a deep Q-network (DQN), with the goal of maximizing its survival time by successfully navigating through pipes. All gameplay data will be generated dynamically during the simulated training process.

### **Tools and Environments**

1. **Game Environment:** PyGame (to create a clone of Flappy Bird).
2. **Programming Language:** Python.
3. **Deep Learning Framework:** PyTorch or TensorFlow for building and training the neural network.
4. **RL Library:** Stable-Baselines3 for implementation of RL algorithms.
5. **Visualization:** Matplotlib and PyGame for tracking progress and generating visual outputs of training performance.

### **Methodology**

### **Game Design and Environment Setup (5 hours)**

- Create or adapt a Flappy Bird clone using PyGame.
- Define the state space: The bird's position, velocity, and the relative positions of the nearest pipe pair.
- Define the action space: Flap (jump) or do nothing.
- Create interfaces to reset the game and step through time, providing rewards and observing new states.

### **Model Architecture (4 hours)**

- Design a deep Q-network with:
    - Input: A fixed-size state vector representing the game environment.
    - Hidden layers: 2-3 fully connected layers with ReLU activation.
    - Output: Q-values for each possible action (flap or no flap).
- Potential hyperparameters:
    - Learning rate: 0.001
    - Discount factor (γ): 0.99
    - Batch size: 32
    - Replay buffer size: 50,000 experiences.

### **Training Strategy (15 hours)**

1. **Exploration vs. Exploitation:**
    - Use ε-greedy exploration with an initial ε of 1.0, decaying over time to 0.1.
    - This ensures that the agent starts by exploring random actions and gradually shifts toward exploiting its learned policy.
2. **Replay Buffer:**
    - Store gameplay experiences (state, action, reward, next state) in a replay buffer.
    - Train the network by sampling mini-batches from the buffer, which helps stabilize learning by breaking correlations between consecutive experiences.
3. **Rewards and Punishments:**
    - Reward: +1 for passing a pipe.
    - Punishment: -1 for hitting a pipe or falling out of bounds.
    - Living reward: 0 (neutral) for staying alive.

### **Progress Tracking (4 hours)**

- Track key metrics during training:
    - Episode length (number of pipes passed).
    - Average reward per episode.
    - Loss values during gradient updates.
- Visualize the agent’s gameplay periodically to observe its progress.
- Generate plots to show the improvement of average scores over time.

### **Testing and Fine-Tuning (4 hours)**

- Evaluate the trained agent on unseen game instances.
- Adjust hyperparameters, reward structure, or model architecture if the performance plateaus or fails to improve.
- Introduce additional game challenges (e.g., varying pipe gaps or speeds) to test robustness.

### **Data Source**

All data for training will be generated by the simulated gameplay environment. The agent interacts with the PyGame Flappy Bird environment to collect states, actions, and rewards dynamically.

### **Time Estimate**

1. **Environment Setup:** 5 hours
2. **Model Architecture Design:** 4 hours
3. **Training and Debugging:** 15 hours
4. **Progress Tracking and Visualization:** 4 hours
5. **Testing and Fine-Tuning:** 4 hours **Total:** ~32 hours (excluding actual training time).

### **Expected Outcome**

The project will produce an RL agent capable of playing Flappy Bird with a strategy learned from scratch. The results will include:

- A trained DQN model.
- Visualizations of the agent’s performance over time.
- Insights into the effectiveness of RL techniques like ε-greedy exploration and replay buffers.

This project offers an engaging application of reinforcement learning and demonstrates the process of training an agent to solve a real-time control problem.

## **Flappy Bird AI Dataset**

### **Dataset Overview**

For this project, I will generate my own dataset by running simulations of a Flappy Bird clone I created using PyGame. Rather than relying on a static, publicly available dataset, the data will be dynamically created as the game is played by an agent. Each “experience” or game step will be recorded as part of the training process of a Deep Q-Network (DQN) reinforcement learning model. The dataset will consist of state-action-reward-next state tuples, often referred to as experiences, which are essential for experience replay in DQN training.

### **Data Generation Process**

The game environment will continuously produce new experiences during gameplay. At each time step, the game will record:

•	**State**: A fixed-size vector representing the environment, including the bird’s vertical position, velocity, the horizontal distance to the next pipe, and the gap height of that pipe.

•	**Action**: A binary action indicating whether the bird flapped (1) or did nothing (0).

•	**Reward**: A numerical value assigned based on survival and score progress (e.g., +1 for passing a pipe, -100 for dying).

•	**Next State**: The state of the environment immediately following the action.

•	**Done**: A Boolean indicating whether the game ended after this step.

As the model plays the game, these experiences will be stored in a replay buffer of a fixed size (e.g., 50,000 most recent experiences). Once the buffer is full, older data will be discarded in favor of new gameplay.

### **Data Cleaning**

Because this dataset is generated in a controlled simulation, traditional cleaning steps like handling missing values, outliers, or inconsistent formats are unnecessary. However, I will monitor the quality of the data by ensuring:

•	State values remain within expected physical ranges (e.g., positions on screen, velocity limits).

•	Rewards are calculated correctly according to the game logic.

•	No corrupt or invalid entries enter the replay buffer (such as incomplete experience tuples).

If irregularities are detected during development (for example, NaN values in states due to game glitches), I will add error handling to discard or correct those experiences.

### **Source of Data**

All data will be generated locally through my custom PyGame Flappy Bird environment.

### **File Format**

When exported, the data will be stored in a structured format such as CSV, where each row represents a complete experience tuple. However, during training, the data will primarily reside in memory within the replay buffer and only be stored between training session as needed.